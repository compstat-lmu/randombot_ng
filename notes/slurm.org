

* Slurm command list to keep in mind

- sacctmgr list cluster
- --cluster=all
- scontrol show config
  - MaxJobCount
  - MaxArraySize
  - MaxStepCount
  - MaxTasksPerNode
  - MaxMemPerNode
  - MaxMemPerCPU
- sacct --job=<jobid>


* Things to keep in mind

- --mem= is PER NODE, but srun *inherits* the value, so srun must be started with --ntask=1, --mem=<less than in sbatch header>
- also --nnodes should be set to 1 to avoid a deluge of warnings
- question: does '--mem=MaxMemPerNode' work? Answer: Yes

* Environment
** Info

- SLURM_STEP_ID (and SLURM_STEPID): step id within job
- SLURM_STEP_NODELIST (list of nodes of step)
- SLURM_CPUS_ON_NODE number of CPUs that were *allocated* for the job step (?)
- SLURM_NTASKS: allocated tasks for job step
- SLURM_NNODES: allocated nodes for job step
- SLURMD_NODENAME: be aware of the "D" ( ͡° ͜ʖ ͡°) : canonical host name of running task

** Example =env= call on mpp3 in lrz:
*** Within SBATCH script

SLURM_CHECKPOINT_IMAGE_DIR=/var/slurm/checkpoint
SLURM_NODELIST=mpp3r02c01s[04-05],mpp3r02c02s[01,03]
SLURM_JOB_NAME=testrun5
SLURMD_NODENAME=mpp3r02c01s04
SLURM_TOPOLOGY_ADDR=coreswitch.switch02.mpp3r02c01s04
HOSTNAME=mpp3r02c01s04
SLURM_PRIO_PROCESS=0
SLURM_NODE_ALIASES=(null)
SLURM_EXPORT_ENV=NONE
SLURM_JOB_QOS=normal
TMPDIR=/tmp
SLURM_TOPOLOGY_ADDR_PATTERN=switch.switch.node
SBATCH_CPU_BIND_LIST=0xFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF
SLURM_NNODES=4
SLURM_JOBID=409506
SLURM_NTASKS=256
SLURM_TASKS_PER_NODE=64(x4)
SBATCH_CPU_BIND_VERBOSE=quiet
SLURM_JOB_ID=409506
SLURM_CPUS_PER_TASK=1
PWD=/home/hpc/pr74ze/di25pic2
SLURM_JOB_USER=di25pic2
SLURM_JOB_UID=3942384
SLURM_NODEID=0
SLURM_SUBMIT_DIR=/home/hpc/pr74ze/di25pic2/projects/lrz
SLURM_TASK_PID=171245
SLURM_NPROCS=256
SLURM_CPUS_ON_NODE=256
SLURM_PROCID=0
ENVIRONMENT=BATCH
SLURM_JOB_NODELIST=mpp3r02c01s[04-05],mpp3r02c02s[01,03]
SHLVL=1
SLURM_LOCALID=0
SLURM_GET_USER_ENV=1
SBATCH_CPU_BIND_TYPE=mask_cpu:
SLURM_JOB_GID=3647528
SLURM_JOB_CPUS_PER_NODE=256(x4)
SLURM_CLUSTER_NAME=mpp3
SLURM_GTIDS=0
SLURM_SUBMIT_HOST=mpp2-login5
SLURM_JOB_PARTITION=mpp3_batch
SLURM_JOB_ACCOUNT=lxcusers
SLURM_JOB_NUM_NODES=4
SBATCH_CPU_BIND=quiet,mask_cpu:0xFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF
SLURM_MEM_PER_NODE=92000
_=/usr/bin/env

   
*** Within Job Step:

SLURM_NODELIST=mpp3r02c01s[04-05],mpp3r02c02s[01,03]
SLURM_CHECKPOINT_IMAGE_DIR=/var/slurm/checkpoint
SLURM_JOB_NAME=testrun5
SLURM_TOPOLOGY_ADDR=coreswitch.switch02.mpp3r02c01s05
SLURMD_NODENAME=mpp3r02c01s05
SLURM_PRIO_PROCESS=0
SLURM_SRUN_COMM_PORT=39281
SLURM_EXPORT_ENV=NONE
SLURM_JOB_QOS=normal
TMPDIR=/tmp
SLURM_TOPOLOGY_ADDR_PATTERN=switch.switch.node
SLURM_CPU_BIND_VERBOSE=quiet
SLURM_CPU_BIND_LIST=0x0000020000000000000002000000000000000200000000000000020000000000
SLURM_NNODES=1
SLURM_STEP_NUM_NODES=1
SLURM_JOBID=409506
SLURM_NTASKS=1
SLURM_LAUNCH_NODE_IPADDR=10.156.64.167
SLURM_STEP_ID=165
SLURM_STEP_LAUNCHER_PORT=39281
SLURM_TASKS_PER_NODE=1
SLURM_CPUS_PER_TASK=1
SLURM_JOB_ID=409506
PWD=/home/hpc/pr74ze/di25pic2
SLURM_STEPID=165
SLURM_JOB_USER=di25pic2
SLURM_SRUN_COMM_HOST=10.156.64.167
SLURM_CPU_BIND_TYPE=mask_cpu:
SLURM_UMASK=0022
SLURM_JOB_UID=3942384
SLURM_NODEID=0
SLURM_SUBMIT_DIR=/home/hpc/pr74ze/di25pic2
SLURM_NPROCS=1
SLURM_TASK_PID=146904
SLURM_DISTRIBUTION=cyclic
SLURM_CPUS_ON_NODE=4
SLURM_PROCID=0
SLURM_JOB_NODELIST=mpp3r02c01s[04-05],mpp3r02c02s[01,03]
SHLVL=1
SLURM_GET_USER_ENV=1
SLURM_LOCALID=0
SLURM_CLUSTER_NAME=mpp3
SLURM_JOB_CPUS_PER_NODE=256(x4)
SLURM_JOB_GID=3647528
SLURM_SUBMIT_HOST=mpp3r02c01s04
SLURM_GTIDS=0
SLURM_JOB_PARTITION=mpp3_batch
SLURM_STEP_NUM_TASKS=1
SLURM_JOB_ACCOUNT=lxcusers
SLURM_JOB_NUM_NODES=4
SLURM_STEP_TASKS_PER_NODE=1
SLURM_STEP_NODELIST=mpp3r02c01s05
SLURM_CPU_BIND=quiet,mask_cpu:0x0000020000000000000002000000000000000200000000000000020000000000
SLURM_MEM_PER_NODE=92000
_=/usr/bin/env
