
* Global settings:
Some of these may be environment-dependent, e.g. NODE_ID, others are global settings that change run behaviour (e.g. SCHEDULING_MODE) or even results (SAMPLING_TRAFO)
** Environment only
- Scheduling
  - ONEOFF :: [string] "TRUE" or "FALSE" (default): whether to restart invocations after each eval (mainly for memory usage profiling).
  - STRESSTEST :: [string] "TRUE" or "FALSE" (default): whether to do dummy invocations and send lots of bogus results to the server to check throughput
  - STARTSEED :: first seed to evaluate. given to *invoke_sbatch.sh* as first argument.
- Redis:
  - REDISHOST :: redis hostname
  - REDISPORT :: redis port
  - DRAINPROCS :: number of processes to start that drain results from redis instance to memory
- R-Script invocation
  - LEARNERNAME :: set in *runscript.sh*, read by eval_redis.R
  - TASKNAME :: set in *runscript.sh*, read by eval_redis.R
** Environment AND R setting
- Directories
  - MUC_R_HOME :: base directory of this repository, has subdirectories =input/=, =R/=, =scheduling/=, =setup/=. Set/inferred by most *.sh* files.
  - DATADIR :: directory where data files are stored, set by *constants.R* to =MUC_R_HOME/data=
** R setting only
- Input settings
  - DATA_TABLE :: filename of data info csv. set by *constants.R*
  - DATA_TABLE_OPTS :: options to be given to =read.csv= when reading DATA_TABLE. set by *constants.R*
  - SEARCHSPACE_TABLE :: filename of search space csv. set by *constants.R*
  - SEARCHSPACE_TABLE_OPTS :: options to be given to =read.csv= when reading SEARCHSPACE_TABLE. set by *constants.R*
  - SEARCHSPACE_PROP_TABLE :: filename of learner proportions csv. set by *constants.R*
  - SEARCHSPACE_PROP_TABLE_OPTS :: options to be given to =read.csv= when reading SEARCHSPACE_PROP_TABLE. set by *constants.R*
- Sampling Configuration
  - SUPERRATE :: [numeric 0..1] fraction of evaluation points that have supererogatory evaluations
  - SUPERCV_PROPORTIONS :: [numeric 0..1] subsampling proportions to sample
  - SAMPLING_TRAFO :: "none", "default", "default+norm"
    - "none" :: transformations given in paramspace csv are not performed (although the given parameter limits are transformed)
    - "default" :: transformations as given in paramspace csv
    - "norm" :: transformation as given, prepended by an inverse error function; parameter bounds as given are instead the inflection points of the normal distribution (i.e. each 1 std-dev from center)
  - RESAMPLINGTIMEOUTS :: [numeric] seconds to wait for each resampling. Violating the time constraint kills the R session if the watchdog is running.
* Directory structure
- data
  input arguments are in file DATADIR/INPUTS, a *single space* separated file with columns <LEARNER> <TASK> <POINT_STRING>. LEARNER changes the fastest, then TASK, then POINT_STRING changes slowest (i.e. LEARNER is the inner loop)
- input
  - learners
- R
- scheduling
- setup
- testenv
* scheduling
Scheduling happens with "Redis". The integer value of key "QUEUE_lrn:<learner>_tsk:<task>" is atomically incremented, the corresponding seed is evaluated, and the result is written to "RESULT_lrn:<learner>_tsk:<task>_SD:<seed>_val:<hyperparameter vals>". Unless ONEOFF is set to TRUE, evaluation happens in an infinite loop until the process is killed. So far there is no way of knowing whether a result is missing because of an error, timeout, memory out, or user intervention.
* Control Flow
1. invoke_sbatch.sh
   - Takes arguments:
     - *JOBCOUNT*
     - ONEOFF
   - Exports further arguments:
     - MUC_R_HOME
   - Does:
     - for loop through *JOBCOUNT*: run *sbatch sbatch.cmd*
2. sbatch.cmd
   - Takes arguments:
     - *MUC_R_HOME*
     - ONEOFF
   - SLURM arguments:
     - SLURM_JOB_NAME (not essential)
     - SLURM_JOB_ID (not essential)
     - *SLURM_NTASKS*
   - Uses from constants.R:
     - *DATADIR*
   - Exports further arguments
     - SBATCH_INDEX :: modified when iterating over CONTROL_JOB_COUNT
     - INDEXSTEPSIZE :: augmented by CONTROL_JOB_COUNT
     - TOTAL_TASK_SLOTS :: indicates total (maximum) number of tasks to queue
     - INDIVIDUAL_TASK_SLOTS :: indicates total (maximum) number of tasks for single invoke_srun instance
   - Does:
     - creates at most SLURM_NTASKS worker threads that repeatedly call srun *runscript.sh*
3. runscript.sh
   - Takes arguments:
     - *TASKNAME* (from arg 1)
     - *LEARNERNAME* (from arg 2)
     - ONEOFF (from arg 3)
   - Exports further arguments:
     - TOKEN
   - Does:
     - calls *eval_redis.R*, also traces the process's memory usage.
4. eval_redis.R
   - Takes arguments:
     - *TOKEN*
     - *MUC_R_HOME*
     - *LEARNERNAME*
     - *TASKNAME*
     - *REDISHOST*
     - *REDISPORT*
     - *ONEOFF*
* Scriptlets


- memory usage info

#+BEGIN_SRC bash


cat ../RESULT_REDIS_3/slurm-48771.out | cut -d : -f 1 | sort | uniq > threads

( echo "dataset learner invocation restart point evalno walltime kernelseconds userseconds cpupercent memorykb" ;
  cat threads | \
  while read t ; do \
    grep -F "$t" ../RESULT_REDIS_3/slurm-48771.out | \
      cut -d ' ' -f 2- | \
        sed 's/\[\[[0-9]\+\]\] ----\[[^]]*\]  exited with status [0-9]*//g' | \
        sed 's/----\[[-0-9:]*_[^]]*\] eval_redis.R//g' | \
        sed 's/----\[[-0-9:]*_[^]]*\] Connecting to redis [^:]*:[0-9]*//g' | \
	sed 's/----\[[-0-9:]*_[^]]*\] Evaluating seed [0-9]*//g' | \	
	sed 's/----\[[-0-9:]*_[^]]*\] Done evaluating seed [0-9]*//g' | \
      tr $'\n' '@' | sed 's/@\([^-![]\)/\1/g' | tr '@' $'\n' | \
      grep -v '^!' | grep 'Evaluating point \|^\[.*kB' | tr $'\n' '@' | \
      sed 's/@\[/ [/g' | tr '@' $'\n' | \
      cut -d ' ' -f 4,5,8,10,12,14,16 | \
      sed 's/[][]//g' | sed 's/kB$//' | sed 's/[%s] / /g' | sed "s/^/$t/" | \
      sed 's/^\[\([^,]*\),\([^,]*\),\([0-9]\+\),\([0-9]\+\)\]/\1 \2 \3 \4 /' | \
      grep -v ')$' ; done
) > memtable

#+END_SRC

#+BEGIN_SRC R

sapply(strsplit(as.character(memtable$walltime), ":"), function(tv) {
  sum((60 ^ seq(length(tv) - 1, 0)) * as.numeric(tv))
})

#+END_SRC

- writing state to disk

#+BEGIN_SRC R

outdir <- "/hppfs/work/pn34jo/di39ram3/RESULT_REDIS_3_PACKAGED"
options(warn=1)

repeat {
  savekeys <- head(unlist(r$KEYS("RESULT_*")), 30000)
  if (length(savekeys) != 30000) {
    cat("clear\n")
    Sys.sleep(60)
    next
  }
  mod1 <- sapply(savekeys, function(x) r$GET(x), simplify = FALSE)
  ret <- parallel::mclapply(split(mod1, 1:30), function(modx) {
    modx <- lapply(modx, unserialize)
    digmod1 <- digest::digest(modx)
#  mod2 <- sapply(savekeys, function(x) unserialize(r$GET(x)), simplify = FALSE)  
#  digmod2 <- digest::digest(mod2)
#  stopifnot(digmod1 == digmod2)
    prefix <- substr(digmod1, 1, 2)
    dir.create(file.path(outdir, prefix), recursive = TRUE, showWarnings = FALSE)
    cat(sprintf("Saving %s\n", digmod1))
    saveRDS(modx, file.path(outdir, prefix, digmod1), compress = FALSE)
    TRUE
  }, mc.cores = 30)
  stopifnot(all(sapply(ret, isTRUE)))
  r$DEL(savekeys)
}

#+END_SRC


#+BEGIN_SRC R

collatedfs <- function(lrname, dfname) {
  memdf <- memtable[memtable$dataset == dfname & memtable$learner == lrname, ]
  rundf <- runinfo[runinfo$dataset == dfname & runinfo$learner == lrname, ]

  stopifnot(all(duplicated(rundf$seed) == duplicated(rundf)))

  rundf <- rundf[!duplicated(rundf), ]

  memdf <- memdf[order(memdf$evalno), ]
  rundf <- rundf[order(rundf$seed), ]

  memdfline <- 1
  rundfline <- 1
  reslist <- list()

  colnames.memdf <- setdiff(colnames(memdf), c("dataset", "learner", "point"))
  colnames.rundf <- setdiff(colnames(rundf), c("dataset", "learner", "point"))

  if (nrow(rundf) == 0) {
    rundf <- rundf[NA, ]
    rundf$dataset <- memdf$dataset[1]
    rundf$learner <- memdf$learner[1]
    rundf$point <- memdf$point[1]
    rundfline <- 2
  }

  if (nrow(memdf) == 0) {
    memdf <- memdf[NA, ]
    memdf$dataset <- rundf$dataset[1]
    memdf$learner <- rundf$learner[1]
    memdf$point <- rundf$point[1]
    memdfline <- 2
  }

  repeat {
    if (memdfline > nrow(memdf)) {
      if (rundfline > nrow(rundf)) {
        break
      }
      remaining <- cbind(memdf[memdfline - 1, ], rundf[seq(rundfline, nrow(rundf)), colnames.rundf])
      remaining$point <- rundf[seq(rundfline, nrow(rundf)), "point"]
      for (makena in colnames.memdf) {
        remaining[seq_len(nrow(remaining)), makena] <- NA  # the seq_len is needed to preserve mode
      }
      reslist <- c(reslist, list(remaining))
      break
    }
    if (rundfline > nrow(rundf)) {
      remaining <- cbind(memdf[seq(memdfline, nrow(memdf)), ], rundf[rundfline - 1, colnames.rundf])
      for (makena in colnames.rundf) {
        remaining[seq_len(nrow(remaining)), makena] <- NA  # the seq_len is needed to preserve mode
      }
      reslist <- c(reslist, list(remaining))
      break
    }
    memdfpoint <- memdf[memdfline, "point"]
    memdfpoint.upcoming <- memdf[seq(memdfline + 1, min(nrow(memdf), memdfline + 50)), "point"]
    rundfpoint <- rundf[rundfline, "point"]
    rundfpoint.upcoming <- rundf[seq(rundfline + 1, min(nrow(rundf), rundfline + 50)), "point"]
    combinedline <- cbind(memdf[memdfline, ],
      rundf[rundfline, colnames.rundf])
    if (memdfpoint == rundfpoint) {
      reslist <- c(reslist, list(combinedline))
      memdfline <- memdfline + 1
      rundfline <- rundfline + 1
      next
    }
    if (memdfpoint %in% rundfpoint.upcoming ||
        (length(rundfpoint.upcoming) < 50 && !rundfpoint %in% memdfpoint.upcoming)) {
      combinedline$point <- rundfpoint
      for (makena in colnames.memdf) {
        combinedline[1, makena] <- NA  # [1, ..] to preserve mode
      }
      reslist <- c(reslist, list(combinedline))
      rundfline <- rundfline + 1
      next
    } 
    if (rundfpoint %in% memdfpoint.upcoming) {
      for (makena in colnames.rundf) {
        combinedline[1, makena] <- NA  # [1, ..] to preserve mode
      }
      reslist <- c(reslist, list(combinedline))
      memdfline <- memdfline + 1
      next
    }
    stop(sprintf("bad configuration: %s %s %s %s", lrname, dfname, memdfline, rundfline))
  }
  resulttable <- do.call(rbind, reslist)
  resulttable$errors.msg <- factor(resulttable$errors.msg, levels = levels(rundf$errors.msg))
  
  if (!anyDuplicated(memdf$point) && !anyDuplicated(rundf$point)) {
    candidate <- merge(x = memdf, y = rundf, by = c("dataset", "learner", "point"), all = TRUE)
    stopifnot(all(colnames(candidate) %in% colnames(resulttable)))
    stopifnot(all(colnames(resulttable) %in% colnames(candidate)))
    stopifnot(isTRUE(all(sort(resulttable$point) == sort(candidate$point))))
    stopifnot(nrow(resulttable) == nrow(candidate))
    stopifnot(!anyDuplicated(resulttable$point))
    candidate <- candidate[match(resulttable$point, candidate$point), colnames(resulttable)]
    attr(candidate, "row.names") <- attr(resulttable, "row.names")
    stopifnot(isTRUE(all.equal(resulttable, candidate)))
  }
  resulttable
}

rxx <- parallel::mclapply(levels(memtable$dataset), function(dfname) {
  do.call(rbind, lapply(levels(memtable$learner), function(lrname) {
    collatedfs(lrname, dfname)
  }))
}, mc.cores = 70)

allruninfo <- do.call(rbind, rxx)

#+END_SRC


** tabulating results

#+BEGIN_SRC R

outdir <- "/hppfs/work/pn34jo/di39ram3/RESULT_REDIS_3_PACKAGED"
resdir <- "/hppfs/work/pn34jo/di39ram3/memanalysis"
options(warn=1)
library("data.table")
library("mlr")

outfiles <- list.files(outdir, recursive = TRUE, full.names = TRUE, include.dirs = FALSE)

result.to.table <- function(filename) {
  content <- readRDS(filename)
  rbindlist(lapply(names(content), function(idn) {
    lname <- gsub("_tsk:.*", "", gsub("RESULT_lrn:", "", idn))
    tname <- gsub("_SD:[0-9].*", "", gsub("RESULT_.*_tsk:", "", idn))
    seed <- as.integer(gsub("_val:.*", "", gsub("RESULT_.*_SD:", "", idn)))
    stopifnot(is.finite(seed) && is.integer(seed))
    point <- gsub(".*_val:", "", idn)
    rres <- content[[idn]]
    stopifnot(isTRUE(rres$learner.id == lname))
    stopifnot(isTRUE(rres$task.id == tname))

    naresults <- aggregate(is.na(rres$pred$data$response), by = list(iter = rres$pred$data$iter), FUN = any)$x
    
    list(
      dataset = tname,
      learner = lname,
      point = point,
      seed = seed,
      evals = nrow(rres$measures.test),
      perf.mmce = performance(rres$pred, list(mlr::mmce)),
      perf.logloss = performance(rres$pred, list(mlr::logloss)),
      traintime = sum(rres$measures.test$timetrain),
      predicttime = sum(rres$measures.test$timepredict),
      totaltime = rres$runtime,
      errors.num = sum(naresults),
      errors.all = all(naresults),
      errors.any = any(naresults),
      errors.msg = c(na.omit(c(t(as.matrix(rres$err.msgs[c("train", "predict")])))), NA)[1]
    )
  }))
}

alltable <- rbindlist(parallel::mclapply(outfiles, result.to.table, mc.cores = 70))

#+END_SRC


#+BEGIN_SRC R

ddx <- data.table::rbindlist(lapply(gsub("=([^-0-9][^,]*),", '="\\1",', alltable$point), function(x) eval(parse(text = x))), fill = TRUE)


#+END_SRC



* TODO

- [-] 200 runs for each learner x task on average, that's a lot.
  - [X] learner-wise data sinks? No: Result Queue
  - [X] raw file writing? No: just lots of drain processes
  - [ ] 512kB/s
  - [ ] test with a bunch of jobs that generate loads of fake data [ implemented: "STRESSTEST" ]
- [X] info to write out
  - [X] write out slurm step number
  - [X] date / time of day
  - [X] give run number to R session as TOKEN and print it
- [X] learner sampling
  - [X] "low discrepancy": number of instances as close to expected number as possible
- [X] stdout / stderr confusion
- [ ] don't write out so much at all
